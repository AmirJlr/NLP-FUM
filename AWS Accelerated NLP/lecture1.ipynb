{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95deeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece5a79",
   "metadata": {},
   "source": [
    "## Stemmer & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c640723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stem(sentence, stemmerName = \"porter\", return_token = False):\n",
    "    \"\"\"\n",
    "    parameters : stemmerName, sentence\n",
    "    - stemmerName :\n",
    "        > porter(~PorterStemmer)\n",
    "        > snow(~SnowballStemmer)\n",
    "        > lancaster (~LancasterStemmer)\n",
    "    - sentence : str \n",
    "    return sentence stemmed, stemmed tokens\n",
    "    \n",
    "    >>> sen, lst = stemmer(\"snow\", \"connecting connection connection\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if stemmerName == \"porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmerName == \"snow\":\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    elif stemmerName == \"lancaster\":\n",
    "        stemmer = LancasterStemmer() \n",
    "        \n",
    "    else :\n",
    "        raise ValueError('please select from valid stemmers [\"porter\", \"snow\", \"lancaster\"]')\n",
    "        \n",
    "    stem_tokens = []\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    for token in tokens:\n",
    "        stem_tokens.append(stemmer.stem(token))\n",
    "        \n",
    "    if return_token == True:\n",
    "        return stem_tokens\n",
    "        \n",
    "    return \" \".join(stem_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b034aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'messag be clean may involv some thing like adjac space tab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stem('messaging be cleaned may involved some thing like adjacenting space tabs',\"snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c8e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(sentence):\n",
    "    \"\"\"\n",
    "    sentence : str\n",
    "    \"\"\"\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    lemma_tokens = []\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    for token in tokens:\n",
    "        lemma_tokens.append(wnl.lemmatize(token))\n",
    "        \n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3420bd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'go', 'went', 'going', 'gon', 'na']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('go goes went going gonna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96964b08",
   "metadata": {},
   "source": [
    "## BOW & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8d7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A major drawback of statistical methods is that they require elaborate feature engineering. \n",
    "Since the early 2010s, the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. \n",
    "Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).\n",
    "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. \n",
    "For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). \n",
    "Latest works tend to use non-technical structure of a given task to build proper neural network\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a12bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c4202",
   "metadata": {},
   "source": [
    "### cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b778cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence_list, sentence_str =False):\n",
    "    \"\"\"\n",
    "    default sentence_list is a list of sentences.\n",
    "    if it was <str> type, put sentence = True\n",
    "    \n",
    "    \"\"\"\n",
    "    if sentence_str == True:\n",
    "        sentence_list = sent_tokenize(sentence_list)\n",
    "        \n",
    "    corpus = []\n",
    "    for sen in sentence_list:\n",
    "        sentence = re.sub(\"[^a-zA-Z]\", \" \", sen)\n",
    "        sentence  = sentence.lower()\n",
    "        sentence = sentence.split()\n",
    "        sentence  = [get_stem(word) for word in sentence if not word in stopwords.words('english')]\n",
    "        sentence = \" \".join(sentence)\n",
    "        corpus.append(sentence) \n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "681cbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"This document is the first document\",\n",
    "             \"This document is the second document\",\n",
    "             \"and this is the third one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c29ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document first document', 'document second document', 'third one']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = clean_text(test)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07a858",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8028486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc4c6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 0],\n",
       "       [2, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfea8aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_binary = CountVectorizer(binary=True)\n",
    "bow_binary = cv_binary.fit_transform(corpus).toarray()\n",
    "bow_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec0556",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a139e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tfidf = tf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87623b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83559154, 0.54935123, 0.        , 0.        , 0.        ],\n",
       "       [0.83559154, 0.        , 0.        , 0.54935123, 0.        ],\n",
       "       [0.        , 0.        , 0.70710678, 0.        , 0.70710678]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c0b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
